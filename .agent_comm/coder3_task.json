{
  "agent_id": "coder3",
  "task_id": "task_7",
  "files": [
    {
      "filename": "prolog_executor.py",
      "purpose": "SWI-Prolog integration layer for executing generated logic programs",
      "priority": "high",
      "dependencies": [
        "pyswip",
        "swiplserver",
        "subprocess",
        "tempfile"
      ],
      "key_functions": [
        "execute_prolog_program",
        "validate_prolog_syntax",
        "extract_tax_result",
        "handle_execution_timeout"
      ],
      "estimated_lines": 300,
      "complexity": "medium"
    },
    {
      "filename": "exemplar_retriever.py",
      "purpose": "Intelligent retrieval system for selecting relevant precedent cases",
      "priority": "high",
      "dependencies": [
        "sentence-transformers",
        "faiss",
        "rank_bm25",
        "numpy",
        "openai"
      ],
      "key_functions": [
        "encode_cases",
        "rank_similar_cases",
        "select_top_k_exemplars",
        "create_few_shot_prompt"
      ],
      "estimated_lines": 400,
      "complexity": "medium"
    }
  ],
  "project_info": {
    "project_name": "NeuroSymbolic_Tax_Reasoning_System",
    "project_type": "nlp",
    "description": "A neuro-symbolic system that combines large language models with a Prolog-based symbolic solver to provide trustworthy, auditable, and cost-effective tax filing assistance. The system translates plain-text tax statutes and taxpayer facts into formal Prolog programs, executes them via SWI-Prolog, and uses intelligent exemplar retrieval to improve parsing accuracy while minimizing real-world error costs.",
    "key_algorithms": [
      "Prolog_Symbolic_Execution",
      "Few_Shot_Semantic_Parsing",
      "Intelligent_Exemplar_Retrieval",
      "Self_Consistency_Checking",
      "Cost_Based_Error_Modeling"
    ],
    "main_libraries": [
      "torch",
      "transformers",
      "swiplserver",
      "numpy",
      "pandas",
      "scikit-learn",
      "sentence-transformers",
      "openai",
      "tiktoken",
      "rank_bm25",
      "faiss",
      "pyswip",
      "pytest",
      "fastapi",
      "uvicorn",
      "pydantic"
    ]
  },
  "paper_content": "PDF: cs.CL_2508.21051v1_Enabling-Equitable-Access-to-Trustworthy-Financial.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nEnabling Equitable Access to Trustworthy Financial Reasoning\nWilliam Jurayj1, Nils Holzenberger2, Benjamin Van Durme1\n1Johns Hopkins University\n2T\u00b4el\u00b4ecom Paris, Institut Polytechnique de Paris\nwjurayj1@jhu.edu\nAbstract\nAccording to the United States Internal Revenue Service,\n\u201cthe average American spends $270 and 13 hours filing their\ntaxes\u201d. Even beyond the U.S., tax filing requires complex rea-\nsoning, combining application of overlapping rules with nu-\nmerical calculations. Because errors can incur costly penal-\nties, any automated system must deliver high accuracy and\nauditability, making modern large language models (LLMs)\npoorly suited for this task. We propose an approach that inte-\ngrates LLMs with a symbolic solver to calculate tax obliga-\ntions. We evaluate variants of this system on the challeng-\ning StAtutory Reasoning Assessment (SARA) dataset, and\ninclude a novel method for estimating the cost of deploying\nsuch a system based on real-world penalties for tax errors. We\nfurther show how combining up-front translation of plain-text\nrules into formal logic programs, combined with intelligently\nretrieved exemplars for formal case representations, can dra-\nmatically improve performance on this task and reduce costs\nto well below real-world averages. Our results demonstrate\nthe promise and economic feasibility of neuro-symbolic ar-\nchitectures for increasing equitable access to reliable tax as-\nsistance.\nIntroduction\n\u201cGPT is not a certified tax professional, nor am I, so\nyou should always check with your tax advisor. \u201d\n\u2014 Greg Brockman, CTO of OpenAI\nOf life\u2019s two certainties, taxes should be preferred; yet\nthey may well be the more complicated one. Each year,\nvirtually every adult in the world must calculate and pay\na fee to some government, in order to reside and earn a\nliving within the state\u2019s guardianship. Even for individu-\nals with relatively simple financial situations, the annual\nfiling process demands meticulous reading and following\nof dozens of form instructions and the copying of values\nacross schedules, worksheets, and eligibility tests. Com-\npleting these tasks without professional assistance can take\nhours. Alternatively, taxpayers may hire a professional pre-\nparer, incurring substantial fees depending on the complex-\nity of their return (Internal Revenue Service 2025).\nAccuracy in tax filing is essential. Over-reported income\nor missed deduction opportunities lead to unnecessary over-\npayment, while under-reporting may result in penalties,\ninterest, and potential legal consequences. In the UnitedStates, the costs of inaccuracies affect lower income commu-\nnities more significantly, in part because these groups offer\nthe Internal Revenue Service (IRS) high audit success rates\n(Black et al. 2022; Elzayn et al. 2025). However, these audits\ndeliver a modest return on investment compared to audits of\nwealthier taxpayers, so there is an opportunity to better align\ncommunity and institutional interests through improved tax\nadvice to lower income taxpayers (Boning et al. 2024).\nFigure 1: A taxpayer confronted with a tax question might\nchoose between an inexpensive AI preparer and a costlier\nhuman professional. The decision considers trade-offs be-\ntween cost, convenience, and confidence in the result.\nHowever, the concrete costs for errors present a substan-\ntial challenge for modern large language models (LLMs).\nAn AI assistant deployed in this domain must meet higher\nstandards than basic accuracy: it should (1) recognize when\nit lacks sufficient certainty to offer guidance and (2) gener-\nate a transparent and faithful trace of logical steps so that\ntaxpayers and auditors can easily verify the derivation of\neach answer. In this paper we show that symbolic reason-\ning tools, integrated with LLMs, offer a promising approach\nto meeting these standards. Our method provides the lan-\nguage model with access to a symbolic solver, enabling it to\ntranslate statutory text and taxpayer information into formal\nlogic programs, which are processed by a trusted execution\nengine. We evaluate the method on the StAtutory Reason-\ning Assessment (SARA) dataset, a benchmark of synthetic\ntax scenarios paired with liability calculations carried out\nthrough ground-truth representations of rules and facts in\nformal logic (Holzenberger et al. 2020).\nOur experiments demonstrate two key findings. First,arXiv:2508.21051v1  [cs.CL]  28 Aug 2025\n\n--- Page 2 ---\nFigure 2: Methods for solving. Top Left: Plain-text for statutes and a case is fed into a language model, along with the\ninstruction to calculate a person\u2019s tax obligation. Top Right: Statutes and a case are fed into the model as before, but it is\ninstructed to convert these into a logic program which calculates a person\u2019s tax obligation. If the SWI-Prolog engine fails\nto execute the program, the case is considered unanswered. Bottom: A language model parses a case\u2019s facts into Prolog,\nconditioned on gold parses of the most relevant cases and of the rules contained in the statutes. The symbolic solver imports the\ngold parses of the statutes before attempting to execute the generated parse of the case. Note that unlike the approaches above\nit, this requires gold symbolic representations of both the statutes and a representative selection of correctly-decided cases.\nwhereas frontier reasoning models outperform non-\nreasoning models at both directly solving and at parsing\ncase and statute text into the symbolic solver, non-reasoning\nmodels consistently outperform their reasoning counterparts\nwhen given gold symbolic representations of statutes and of\ntheir application to similar cases. Second, we show that by\nadding additional refusal criteria through a symbolic solver\nand self-checking, the expected costs of deploying such\na system in the real world could be brought down to less\nthan 20% of the average cost for an American to file their\ntaxes. Our results indicate the promise of neuro-symbolic\narchitectures for expanding access to trustworthy and\nreliable tax expertise.\nBackground\nLogic Programming for Legal Reasoning\nSeveral programming languages have been designed to rep-\nresent and facilitate logical reasoning. Prolog is a declara-\ntive programming language for representing and reasoning\nover knowledge, with roots in first order logic. A program-\nmer defines rules using Horn clauses (Horn 1951) and facts\nby declaring which rules apply to entities, thus populating a\nknowledge base. Subsequently, this knowledge base can be\nqueried by defining a \u2018goal\u2019, which launches computation in\nthe form of a backward-chaining search attempting to prove\nthat the goal holds a certain value (Wielemaker et al. 2010).\nProlog has been used since the early days of legal AI, where\nit has formed the backbone of legal expert systems because\nits declarative syntax keeps knowledge base entries for ruleshuman-readable while powerfully representing the reason-\ning around which legal questions revolve (Sherman 1989).\nEfforts in countries like the United Kingdom (Sergot et al.\n1986), Canada (Sherman 1987), and the United States (Kant\net al. 2025) have leveraged this capacity to encode legal rules\nin executable formal logic. Related languages have also been\nused in the legal domain, such as Answer-set Programming\n(Gelfond and Lifschitz 1988; Morris 2020), Datalog (Ceri\net al. 1989; Huang et al. 2021), and Catala (Merigoux et al.\n2021a; Merigoux 2023). More broadly, hierarchical tem-\nplates are a popular tool for evaluation of legal reasoning\n(Hou et al. 2024). We focus on the SARA dataset (Holzen-\nberger et al. 2020), which encodes statutes and cases into\nProlog logic programs to show how a symbolic expert sys-\ntem can perfectly solve a task which large language models\nstruggle to complete.\nStatutory Tax Reasoning and the SARA Dataset\nWe focus on the task of statutory reasoning for tax law.\nSome elements of this task bear similarity to popular math-\nematical reasoning tasks such as GSM-8k (Cobbe et al.\n2021) or MATH-500 (Hendrycks et al. 2021), such as chain-\ning together mathematical operations to solve a real-world\nproblem described in words. However, unlike these math\ndatasets which require application of a small set of universal\narithmetic rules which models learn during training, statu-\ntory reasoning considers a set of contingent rules contained\nwithin documents provided to a model in-context at infer-\nence time, in addition to these basic arithmetic principles.\nWe evaluate our methods on the SARA dataset, which\n\n--- Page 3 ---\nFigure 3: Number of correct and incorrect solutions pro-\nduced by each solution method, for large chat- and\nreasoning-optimized models (served by DeepSeek and Ope-\nnAI).\ntests the ability of language models to do statutory reasoning\nabout the United States Tax Code (Holzenberger et al. 2020).\nThis dataset is included in the popular aggregate benchmark\nLegalBench (Guha et al. 2023), and was used in the GPT-\n4 product launch to highlight the model\u2019s superior reason-\ning capacity (Blair-Stanek et al. 2024). The SARA dataset\nconsists of 9 sections from the US federal tax code which\nhave been moderately edited to make them self-contained\nand unambiguous. These manipulations allow the dataset\nto serve as a self-contained and solvable task for language\nmodels that lack live internet access and human-like abili-\nties to process ambiguity (Jurayj et al. 2022; Stengel-Eskin\net al. 2024). These statute sections are accompanied by 376\nhand-crafted cases to test understanding of these statutes,\neach containing a question about a person\u2019s tax obligation.\nEach statute and case has been manually translated into Pro-\nlog, which allows them to be trivially solved using the lan-\nguage\u2019s powerful execution engine to resolve queries about\ncases. This Prolog is defined using neo-Davidsonian event\nsemantics (Davidson 1966), thus categorizing each event as\none of 61 possible predicates onto which various arguments\nare attached. Of the 376 cases and corresponding questions,\n276 require binary judgments about whether a statute ap-\nplies to a given case, and 100 require numerical judgments\nabout how much tax a person owes in a given year. We focus\non these 100 tax cases because of their increased difficulty,\nand because a trivial baseline of always guessing a single an-\nswer delivers poor performance at predicting the numerical\noutput.\nZero-Shot Solving from Statutory Text\nDirect Calculation\nWe evaluate our methods against the baseline method of di-\nrect solving, mirroring the approach used in OpenAI\u2019s GPT-\n4 demonstration (Blair-Stanek et al. 2024). This approach\ntreats the tax calculation as a mathematical question answer-\ning task with the additional demand that the model must\napply entries from a large corpus of rules contained in the\nstatutory text, in addition to the generic arithmetic rulesthat govern all calculation. For each case in this setting, a\nmodel\u2019s context is filled with all sections of the statutes con-\ncatenated together, the description of the case\u2019s facts, and the\nquestion about a person in that case\u2019s tax liability, all in plain\ntext. It is instructed to calculate the person in question\u2019s tax\nobligation based on the rules outlined in the statutes.\nChat Model Reasoning Model Size\nQwen2.5-Coder R1-Distill Qwen2.5 32 billion\nLlama 3.3 R1-Distill Llama 3.3 70 billion\nDeepSeek-V3 DeepSeek-R1 671 billion\nGPT-4.1 OpenAI o3 $8/m tokens\nTable 1: Chat and Reasoning model pairs. Each open-\nweight model pair is fine-tuned from the same base model.\nAlthough the exact dimensions and provenance of OpenAI\u2019s\nmodels are unknown, the two models have identical token\npricing structures, suggesting that they incur similar costs\nfor OpenAI to serve.\nCalculation by Parsing for a Symbolic Solver\nTo extend the direct solution approach, we augment the lan-\nguage model with a symbolic solver. Here, a model is given\nthe plain text of the statutes as in the direct calculation case.\nIt is instructed to generate a Prolog program which encodes\nthe relevant rules and facts necessary to compute the per-\nson in question\u2019s tax obligation. The symbolic solver ingests\na set of rules and facts in Prolog, and is invoked to exe-\ncute a query. The execution of this Prolog program offers a\nstraightforward mechanism for refusal: if the program fails\nto execute into the proper format or hangs beyond a pre-\nallocated time limit (10 seconds), the system is considered\nto have refused to answer.\nExperimental Setup\nWe run experiments across four model families of different\nsizes, three of which are open-weight models. The bases\nfor these models are: Qwen2.5 32B (Qwen et al. 2025),\nLlama 3.3 70B (Grattafiori et al. 2024), DeepSeek-V3 671B\n(DeepSeek-AI et al. 2025b), and OpenAI\u2019s GPT-4.1 (Ope-\nnAI 2025a,b), each of which has an instruction-tuned ver-\nsion designed for common chat applications, and a reason-\ning version optimized to expend additional inference-time\ncompute to solve harder problems. The full list of models\nis included in table 1. We run auxiliary experiments using\nGPT-5, but we do not conduct the same chat vs. reasoning\ncomparison because this product appears to be an integrated\nsystem containing several models, and therefore is not as\nanalogous to these other comparisons (Zhang et al. 2025).\nThe reasoning model for three of these pairings stem from\nthe DeepSeek R1 project (DeepSeek-AI et al. 2025a), where\nstrong base models were fine-tuned to generate long chains-\nof-thought that help them solve harder quantitative reason-\ning problems. Although there is no formal documentation\nstating that OpenAI\u2019s GPT-4.1 and o3 models are derived\nfrom the same base model, the proximity of the two model\u2019s\nlaunch dates and their identical per-token pricing schemes\n\n--- Page 4 ---\nModel Family Model Method Correct Incorrect Abstentions Break-Even Price\nBaselineN/A Always Abstain 0 0 100 $270 \u00b1 0\nN/A Always Predict $0 5 95 0 $16227.11 \u00b1 7805.94\nQwen-2.5Qwen-32b Direct 13 87 0 $3,051.64 \u00b1 1,828.31\nQwen-32b Parsed 2 17 81 $490.34 \u00b1 230.75\nR1-32b Direct 38 62 0 $505.25 \u00b1 287.67\nR1-32b Parsed 1 2 97 $278.70 \u00b1 24.33\nLlama-3.3Llama-70b Direct 9 91 0 $1,065.90 \u00b1 675.07\nLlama-70b Parsed 1 43 56 $252,027.73 \u00b1 414,049.97\nR1-70b Direct 43 57 0 $1,257.03 \u00b1 1,620.47\nR1-70b Parsed 2 1 97 $266.10 \u00b1 6.81\nDeepSeekDeepSeek-V3 Direct 22 78 0 $739.45 \u00b1 474.59\nDeepSeek-V3 Parsed 11 43 46 $2,099.13 \u00b1 1,253.57\nDeepSeek-V3 Direct + Direct 16 15 69 $265.46 \u00b1 63.53\nDeepSeek-V3 Direct + Parsed 7 4 89 $285.53 \u00b1 55.57\nDeepSeek-V3 Parsed + Parsed 5 8 87 $310.47 \u00b1 67.95\nDeepSeek-R1 Direct 74 26 0 $304.29 \u00b1 225.57\nDeepSeek-R1 Parsed 38 10 52 $249.64 \u00b1 84.77\nDeepSeek-R1 Direct + Direct 66 12 22 $94.20 \u00b1 59.76\nDeepSeek-R1 Direct + Parsed 34 3 63 $170.10 \u00b1 21.75\nDeepSeek-R1 Parsed + Parsed 17 4 79 $241.80 \u00b1 29.45\nOpenAI GPT-4.1GPT-4.1 Direct 48 52 0 $532.84 \u00b1 492.99\nGPT-4.1 Parsed 39 31 30 $228.89 \u00b1 151.69\nGPT-4.1 Direct + Direct 42 13 45 $196.92 \u00b1 88.43\nGPT-4.1 Direct + Parsed 27 6 67 $185.10 \u00b1 21.33\nGPT-4.1 Parsed + Parsed 26 5 69 $186.30 \u00b1 20.84\no3 Direct 56 44 0 $6,431.84 \u00b1 2,637.94\no3 Parsed 75 15 10 $47.43 \u00b1 22.16\no3 Direct + Direct 41 17 42 $3,472.29 \u00b1 1,859.32\no3 Direct + Parsed 52 10 38 $115.90 \u00b1 24.63\no3 Parsed + Parsed 65 9 26 $77.51 \u00b1 22.41\nOpenAI GPT-5GPT-5 Direct 76 24 0 $299.11 \u00b1 288.41\nGPT-5 Parsed 53 13 34 $122.72 \u00b1 29.21\nGPT-5 Direct + Direct 73 9 18 $218.64 \u00b1 270.19\nGPT-5 Direct + Parsed 46 6 48 $138.30 \u00b1 25.53\nGPT-5 Parsed + Parsed 31 5 64 $180.23 \u00b1 23.42\nTable 2: Results of different methods without gold statutes. Models in the same family have the same base model (or seem\nmost likely to, in the case of closed-weights models). Note that \u201cbreak-even price\u201d measures only the costs of failures and\nabstentions, and does not include inference costs. For each model, the approach that delivered the lowest break-even price is\nshown in bold. The top two rows show the break-even price of trivial systems, which always defer to an expert or which always\ntell a person not to pay any taxes. Errors represent a 90% confidence interval. The lowest break-even price method for each\nmodel family is in bold.\nsuggest that these two models have a similar relationship to\nthe other model pairs we explore.\nWe consider \u2018correct\u2018 attempts to be those where the out-\nput calculated by our system is exactly the same as the ac-\ntual tax obligation, when rounded to the nearest dollar. All\nProlog code is executed using the SWI-Prolog (Wielemaker\net al. 2010) implementation of Prolog, and externally halted\nafter 10 seconds of reasoning. We run experiments on the v2\nrelease of the SARA dataset, because its programmatic rep-\nresentations most closely match the natural language surface\nforms (Holzenberger and Van Durme 2021).Self-Consistency Tests\nWe further ask how effectively these methods can serve to\nimprove each other\u2019s selectivity, by using comparisons be-\ntween different solution methods to expend additional com-\npute to help determine whether an answer should be trusted\n(Wang et al. 2023; Stengel-Eskin and Van Durme 2023; Ju-\nrayj et al. 2025). In these settings, an answer is only ac-\ncepted if it is reached via two independent reasoning pro-\ncesses: two chains-of-thought and answers (either directly\ncalculated obligations or Prolog programs) are sampled from\nthe same model. When self-checking using the same method\n(for instance, \u201cParsed + Parsed\u201d), these answers are condi-\ntioned on the same prompt and context. In the parsing-based\n\n--- Page 5 ---\napproaches, this can be considered a more stringent ver-\nsion of the existing refusal system which rejects questions\nfor which the parsed solution does not execute, by addition-\nally deferring to a tax professional where a combination of\nattempts do not reach a consensus answer. We test the ef-\nfectiveness of each combination of reasoning processes for\neach model to show where additional selectivity can further\nimprove performance.\nIncorporating Costs of Incorrect Judgments\nTo file a faulty tax return can incur substantial financial\ncosts. These can take the form of government-imposed\npenalties for understatement, or simply the cost of paying\nmore in tax than one actually owes. Previous evaluations of\nSARA typically report exact match scores on the tax cases.\nRecent work has examined how large the errors are between\nthe obligations calculated by a language model, observing\nthat smaller errors are more frequent than larger ones (Blair-\nStanek et al. 2024). We extend this approach by calculating\nthe costs that would be incurred by using these systems to\nfile the taxes for the 100 tax cases in the SARA dataset.\nWe posit that the deployment of an automated tax advi-\nsor system should require some accountability for the orga-\nnization deploying it. To provide a realistic estimate of the\ncosts of employing the methods we outline above draw from\nthe US Internal Revenue Code (IRC) \u00a76662 , which imposes\na penalty of 20% of the amount underpaid for \u201csubstantial\nunderstatement of income tax\u201d. The cases in SARA are all\npersonal or household tax cases, so a tax filing is consid-\nered a \u201csubstantial understatement\u201d if they are less than the\nmaximum of 10% of the actual amount owed or $5,000. We\nfinally impose a penalty of $270 for refusing to answer, to\nmodel the cost of the personal or professional time required\nto complete one\u2019s taxes (Internal Revenue Service 2025).\nThis gives us the following formula: Let Nbe the number\nof cases the system ingests, and \u2206yi=yi\u2212\u02c6yibe the differ-\nence between the actual obligation yiand predicted obliga-\ntion\u02c6yi, such that positive values of \u2206yiindicate understate-\nment and negative values indicate overstatement.\ncost=1\nNNX\ni=1\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\u2212(\u2206yi),\u2206yi<0,\n0.2 (\u2206yi),\u2206yi>max\u0000\n$5,000,0.1\u00b7yi\u0001\n,\n$270, if refused ,\n$0, otherwise .\nThe first line corresponds to the cost of overstatement (i.e.\nthe amount overstated), the second and fourth lines corre-\nspond to the respective fees incurred for substantial and non-\nsubstantial understatements, while the third line simulates\nthe average cost for an American to file their taxes accord-\ning to the IRS (Internal Revenue Service 2025).\nThis cost also corresponds to the break-even price of the\ntax assistant, i.e. the minimum price at which they might of-\nfer this service without becoming insolvent. This simulates\na real-world scenario in which an organization assumes lia-\nbility for the costs of the errors their system makes for theirusers, and offers tax filing with deferral to a tax expert at a\nfixed price point. Here, the break-even price would inform\nthe minimum price at which they might offer this service\nwithout becoming insolvent, such that a more accurate sys-\ntem delivers a lower break-even price. LLM inference costs\nremain under $1 per-question, and are omitted because of\ntheir marginal impact.\nWe note that the penalty discussed can also be imposed for\n\u201cnegligence or disregard of rules or regulations\u201d while filing\ntaxes. One interpretation might be that using an AI system\nto complete one\u2019s taxes is inherently negligent, but for the\npurposes of this work we assume this is not the case.\nExemplars Successes Failures Break-Even Price\nRanked 87 8 $247.99\nRandom 71 24 $5459 .25\nTable 3: Intelligently re-ranking few-shot exemplars im-\nproves performance. We measure the advantage of intelli-\ngent versus random exemplar re-ranking on GPT-4.1.\nFew-Shot Parsing using Gold Statutes\nWe additionally consider what advantage could be given by\noffering language model parsers access to gold symbolic\nrepresentations of the statutes, alongside demonstrations of\ncases applying the rules from these statutes, building on\nprevious work showing how demonstrations can aid in se-\nmantic parsing (Shin and Van Durme 2022; Spiegel et al.\n2024). Notably, the gold parsed cases all reference the same\nmanually-translated Prolog representation of the statutes. As\nsuch, these manual translations formalize certain types of\nfacts in one particular way where several may be viable,\ngiven alternative representations of the rules which are plau-\nsible but not implemented in practice. Thus, when using al-\nready parsed cases as few-shot examples, the execution en-\ngine must have access to this particular formalization of the\nrules as well.\nFigure 4: Success and failure rates of method mixtures:\nThe top right corner counts the average number of successes\nyielded by each method combination, and the bottom left\ncorner counts the average number of failures for models over\n100 billion parameters optimized for reasoning (DeepSeek\nR1 and OpenAI o3) and chat (DeepSeek V3 and GPT-4.1)\nTo identify the most salient exemplar cases, we apply in-\nformation retrieval systems which can follow instructions\n\n--- Page 6 ---\n(Weller et al. 2025a), directing a retrieval system to rank\ncases based on how similar the logical structure of the case\u2019s\ntext is to the case at hand. For each case, we instruct a\nlightweight reasoning model (OpenAI o4-mini) to rank the\nother 99 cases, following recent work showing the effective-\nness of test-time scaling for reranking documents (Weller\net al. 2025b; Yang et al. 2025). As few-shot examples, we\nprovide the 5 most relevant cases and their gold Prolog\ntranslations in-context for the language model to condition\nits parse on. These cases can be analogized to precedents,\nbecause our tax calculation agent uses them to understand\nhow terms from the statutes are applied in practice. We note\nthat this use of the term \u2018precedent\u2019 is informal and does\nnot refer to the common-law practice of binding precedent.\nRather, these retrieved precedent cases are more analogous\nto \u2018persuasive\u2019 precedent, which might help a court under-\nstand how terms have been applied in the past but is not itself\nnew law which future courts must apply (Kozel 2014).\nTo test the value of this intelligent selection process for\nfew-shot examples, we compare the performance of GPT-\n4.1, which table 4 shows is the strongest model in this set-\nting, against its performance with randomly selected few-\nshot examples in table 3. Although the model delivers strong\nperformance when conditioned even on random exemplars,\nits performance is substantially improved through the intel-\nligent exemplar retrieval process.\nResults\nWe display the effectiveness of each method which doesn\u2019t\nuse the gold statutes in table 2. To extend these, we show\nthe effectiveness of methods which access gold statutes and\nexemplars in table 4, although we note that these are not\ndirectly comparable to the results in table 2 because they\nrequire more up-front human effort.\nWe observe a substantial divergence between the effec-\ntiveness of reasoning- and chat-optimized models on dif-\nferent variants of this task. We visualize this in fig. 3, ag-\ngregating performance of the more powerful DeepSeek and\nOpenAI models optimized for Chat and Reasoning. Figure 4\nshows how this disparity is further amplified when an an-\nswer must be reached via two independent reasoning paths\nin order to be accepted; whereas reasoning models can effec-\ntively mix methods to check their work, chat models deliver\nexceptional performance when self-checking few-shot solu-\ntions, but weaker performance otherwise.\nDiscussion\nOur results indicate the promise of augmenting large lan-\nguage models with a symbolic solver. In both settings with\nand without access to gold symbolic representations of the\nstatutes, the most effective method combined the strongest\nmodel (i.e. the OpenAI offering) with the Prolog symbolic\nsolver. In addition to this performance advantage, the use of\nthe symbolic solver is desirable for this specific task because\nit means that taxpayers or auditors may inspect and debug\nthe system\u2019s reasoning process after the fact, with a guar-\nantee that the decision the system reached was achieved by\nthe path which the logic program articulates. Although one\nFigure 5: Cases with more words are more likely to be\nmiscalculated. We note a moderate correlation between the\ncase length and the failure rate of solution attempts.\nmight attempt to audit the chains of thought used to solve\nthe problem in the direct solution cases, the causal relation-\nship between these chains and the answer they help produce\nis less robust than symbolic program execution, and can be\ndeeply misleading to human readers (Paul et al. 2024; Barez\net al. 2025; Li et al. 2025; Skaf et al. 2025).\nInterestingly, our experiments aggregated in fig. 3 reveal\na notable divergence between models optimized for reason-\ning and for chat applications. Although reasoning models\nperform better at direct solving and zero-shot parsing into\nsymbolic representations of rules and facts, the chat mod-\nels exhibit surprising effectiveness at few-shot parsing of\ncase facts. It is possible that reasoning models\u2019 post-training\nfocuses their efforts on emulating explicit reasoning steps,\nsuch as those Prolog would itself execute, rather than ac-\ncurately mapping input texts into symbolic form. Whereas\nlong chains of thought are useful for the complicated arith-\nmetic required for directly calculating a tax obligation or for\ndiscerning which portions of the statutes should be included\nin a zero-shot parse, they degrade performance on the rela-\ntively simpler task of imitating exemplary conversions from\nnatural language into formal logic. This could be a manifes-\ntation of the \u201clost in the middle\u201d effect in long-context pro-\ncessing (Liu et al. 2024): when the model is producing its\nfinal parse, the case\u2019s plain-text is in between a long block\nof the statutes\u2019 Prolog and the long chain of thought which\nthe model has just generated, thus distracting the model from\nthe relatively straightforward task of converting from natural\nlanguage into the neo-Davidsonian Prolog syntax which can\ncomprehensively represent the SARA cases.\nIn practice, this additionally means that with gold ex-\nemplars of case and statute translations, an individual\u2019s tax\ncase could be processed much more quickly than it could\nbe without these manual annotations, because chat models\ntake dramatically less time than reasoning models to pro-\nduce answers. This could help build more interactive tax as-\nsistance tools which allow users to quickly iterate to amend\n\n--- Page 7 ---\nModel Family Model Method Successes Failures Abstentions Break-Even Price\nPrevious Best o3 Parsed 75 15 10 $47.43 \u00b1 22.16\nQwen-2.5Qwen-32b Few-Shot 42 38 20 $4,676.49 \u00b1 4,623.77\nR1-32b Few-Shot 47 33 20 $7,783.29 \u00b1 6,244.96\nLlama-3.3Llama-70b Few-Shot 70 27 3 $1,917.32 \u00b1 1,247.50\nR1-70b Few-Shot 29 57 14 $6,328.48 \u00b1 2,545.70\nDeepSeekDeepSeek-V3 Few-Shot 78 18 4 $468.66 \u00b1 273.47\nDeepSeek-V3 Direct + Few-Shot 18 2 80 $223.43 \u00b1 19.69\nDeepSeek-V3 Parsed + Few-Shot 9 1 90 $250.43 \u00b1 15.31\nDeepSeek-V3 Few-Shot + Few-Shot 73 9 18 $271.45 \u00b1 230.09\nDeepSeek-R1 Few-Shot 40 16 44 $378.73 \u00b1 155.14\nDeepSeek-R1 Direct + Few-Shot 32 2 66 $178.20 \u00b1 21.34\nDeepSeek-R1 Parsed + Few-Shot 19 2 79 $234.65 \u00b1 28.76\nDeepSeek-R1 Few-Shot + Few-Shot 20 3 77 $215.33 \u00b1 20.63\nOpenAI GPT-4.1GPT-4.1 Few-Shot 87 8 5 $247.99 \u00b1 341.76\nGPT-4.1 Direct + Few-Shot 47 0 53 $143.10 \u00b1 22.49\nGPT-4.1 Parsed + Few-Shot 38 1 61 $164.70 \u00b1 21.98\nGPT-4.1 Few-Shot + Few-Shot 81 5 14 $40.08 \u00b1 15.87\no3 Few-Shot 81 13 6 $60.26 \u00b1 58.93\no3 Direct + Few-Shot 51 5 44 $126.41 \u00b1 24.54\no3 Parsed + Few-Shot 68 7 25 $75.11 \u00b1 22.46\no3 Few-Shot + Few-Shot 74 8 18 $58.13 \u00b1 20.91\nOpenAI GPT-5GPT-5 Few-Shot 86 9 5 $15.78 \u00b1 10.34\nGPT-5 Direct + Few-Shot 71 5 24 $64.98 \u00b1 19.23\nGPT-5 Parsed + Few-Shot 50 2 48 $129.60 \u00b1 22.51\nGPT-5 Few-Shot + Few-Shot 84 6 10 $29.28 \u00b1 13.84\nTable 4: Results of different methods with access to gold statutes and intelligently retrieved parsing exemplars. Columns\nhave the same meaning as table 2. Note that because these results require the additional work of manually translating all statutes\nand a set of representative cases, they are not directly comparable to those in table 2. The top row shows the best previous result.\nand clarify their relevant tax information. This approach no-\ntably reduces the expected real-world cost per successful tax\nfiling, highlighting the critical role that intelligent exemplar\nselection can play in guiding model outputs toward faith-\nful and structured reasoning. The most effective configura-\ntion at minimizing the system\u2019s break-even price is achieved\nwhen providing models with intelligently retrieved prece-\ndential examples, and deferring to a tax expert unless this\nsystem yields the same answer from two independently sam-\npled solutions. The break-even price of $49.48for this sys-\ntem would save the average American taxpayer over 80%\nof the amount spent on tax filing (Internal Revenue Service\n2025). This strong performance could be a strong first step\ntowards real-world pilot studies.\nOf course, to expect that statutory codes would be manu-\nally translated into a logic programming language like Pro-\nlog is a constraining assumption to deliver this strong per-\nformance. However, we note that a handful of existing or-\nganizations have embarked on this task, such as projects to\nencode the French tax code (Merigoux et al. 2021b) or Cana-\ndian policy proposals (Morris 2020) into logic programs. In\nthe United States, private companies build similar logic pro-\ngrams into the backend of consumer tax arrangement soft-\nware (Yu et al. 2020). As this practice increases in popular-\nity, it will expand opportunities to implement methods which\nuse gold-standard programmatic representations of rules.We look closely at the cases which are most and least\ncommonly solved correctly by the most powerful models\n(DeepSeek and OpenAI). We see in fig. 5 that cases which\nhave more words in them are more likely to be mistaken\nacross models, perhaps because they are more likely to con-\ntain intricate logical structures which are more difficult to\ntranslate. For instance, table 5 shows that both case 91 and\ncase 83 were miscalculated by every approach. These two\ncases have an unusual predicate structure that does not ap-\npear in other cases, involving the person in question hir-\ning overlapping groups of people for different periods of\nthe year. Although either case appears in the other case\u2019s\nfew-shot examples when these are selected by the intelligent\nreranking system, the single example appears insufficient for\nany of the models we test to grasp the application of this un-\nusual logical structure. Real-world deployment should con-\nsider how additional heuristics for case difficulty can further\ninform the decision to defer to a tax professional.\nFinally, we note in table 2 a dramatic jump in performance\nwith scale at parsing cases and statutes without gold trans-\nlations. Although smaller reasoning models (those derived\nfrom Qwen2.5 32B and Llama 3.3 70B) perform solidly at\ndirect solving, and the smaller chat models perform solidly\nat few-shot parsing, neither category of smaller model solves\nmore than a handful of cases in the zero-shot parsing set-\nting. In contrast, all larger models solve at least 10% of\n\n--- Page 8 ---\nLeast Frequently Failed Most Frequently Failed\nCase Index Errors Case Index Errors\ntaxcase 89 0 taxcase 26 12\ntaxcase 11 1 taxcase 47 12\ntaxcase 76 1 taxcase 10 12\ntaxcase 45 1 taxcase 91 12\ntaxcase 56 2 taxcase 83 12\nTable 5: Most and least frequently mistaken cases.\nthese cases correctly, while for OpenAI o3 this is the most\nsuccessful setting. We hope that further increases in model\nscale will further improve performance in this setting, since\nit combines the low human cost of direct solving with the\neasy auditing of systems that rely on the symbolic solver.\nConclusion\nWe show how the integration of symbolic solvers with fron-\ntier language models can enhance the capability of AI tax\nassistance systems, and highlight their potential to improve\nequitable access to accurate and affordable tax guidance. Al-\nthough tradeoffs remain between up-front costs of translat-\ning rules into formal logic versus ongoing inference-time\ncomputational and error costs, leveraging symbolic reason-\ning reduces overall expenses while improving auditability in\nboth cases. Future research will explore how further scale or\nspecialized smaller models optimized for faithful translation\ncan improve aspects of this task, and develop methods for ef-\nficient translation of statutory rules into formal logic to en-\nable effective few-shot learning. Finally, real-world imple-\nmentation should be facilitated through interactive user stud-\nies using a further optimized system, enabling widespread\naccessibility to trusted, cost-efficient AI tax assistance.\nReferences\nFazl Barez, Tung-Yu Wu, Iv \u00b4an Arcuschin, Michael Lan,\nVincent Wang, Noah Siegel, Nicolas Collignon, Clement\nNeo, Isabelle Lee, Alasdair Paren, Adel Bibi, Robert Trager,\nDamiano Fornasiere, John Yan, Yanai Elazar, and Yoshua\nBengio. 2025. Chain-of-Thought Is Not Explainability.\nArXiv .\nEmily Black, Hadi Elzayn, Alexandra Chouldechova, Jacob\nGoldin, and Daniel Ho. 2022. Algorithmic Fairness and Ver-\ntical Equity: Income Fairness with IRS Tax Audit Models.\nIn2022 ACM Conference on Fairness Accountability and\nTransparency , pages 1479\u20131503, Seoul Republic of Korea.\nACM.\nAndrew Blair-Stanek, Nils Holzenberger, and Benjamin\nVan Durme. 2024. OpenAI Cribbed Our Tax Exam-\nple, But Can GPT-4 Really Do Tax? arXiv preprint .\nArXiv:2309.09992 [cs].\nWilliam C Boning, Nathaniel Hendren, Ben Sprung-Keyser,\nand Ellen Stuart. 2024. A welfare analysis of tax audits\nacross the income distribution*. The Quarterly Journal of\nEconomics , 140(1):63\u2013112.\nS. Ceri, G. Gottlob, and L. Tanca. 1989. What you always\nwanted to know about datalog (and never dared to ask).IEEE Transactions on Knowledge and Data Engineering ,\n1(1):146\u2013166.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark\nChen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\nTworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training Verifiers to Solve Math\nWord Problems. arXiv preprint . ArXiv:2110.14168 [cs].\nDonald Davidson. 1966. The Logical Form of Action Sen-\ntences. In Donald Davidson, editor, Essays on Actions and\nEvents , page 0. Oxford University Press.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai\nYu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu\nLi, Ziyi Gao, and 181 others. 2025a. DeepSeek-R1: Incen-\ntivizing Reasoning Capability in LLMs via Reinforcement\nLearning. arXiv preprint . ArXiv:2501.12948 [cs].\nDeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan\nWang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo,\nDejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun\nLin, Fucong Dai, and 181 others. 2025b. DeepSeek-V3\nTechnical Report. arXiv preprint . ArXiv:2412.19437 [cs].\nHadi Elzayn, Evelyn Smith, Thomas Hertz, Cameron\nGuage, Arun Ramesh, Robin Fisher, Daniel E Ho, and Ja-\ncob Goldin. 2025. Measuring and Mitigating Racial Dispar-\nities in Tax Audits*. The Quarterly Journal of Economics ,\n140(1):113\u2013163.\nMichael Gelfond and Vladimir Lifschitz. 1988. The stable\nmodel semantics for logic programming. In Proceedings of\nInternational Logic Programming Conference and Sympo-\nsium , pages 1070\u20131080. MIT Press.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Ab-\nhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy\nYang, Angela Fan, Anirudh Goyal, Anthony Hartshorn,\nAobo Yang, Archi Mitra, Archie Sravankumar, Artem Ko-\nrenev, Arthur Hinsvark, and 542 others. 2024. The Llama 3\nHerd of Models. arXiv preprint . ArXiv:2407.21783 [cs].\nNeel Guha, Julian Nyarko, Daniel E. Ho, Christopher\nRe, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood,\nAustin Peters, Brandon Waldon, Daniel Rockmore, Diego\nZambrano, Dmitry Talisman, Enam Hoque, Faiz Surani,\nFrank Fagan, Galit Sarfaty, Gregory M. Dickinson, Hag-\ngai Porat, Jason Hegland, and 21 others. 2023. Legalbench:\nA collaboratively built benchmark for measuring legal rea-\nsoning in large language models. In Thirty-seventh Con-\nference on Neural Information Processing Systems Datasets\nand Benchmarks Track .\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. 2021. Measuring mathematical problem solving\nwith the MATH dataset. In Thirty-fifth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks\nTrack (Round 2) .\nNils Holzenberger, Andrew Blair-Stanek, and Benjamin\n\n--- Page 9 ---\nVan Durme. 2020. A Dataset for Statutory Reasoning in\nTax Law Entailment and Question Answering. San Diego .\nNils Holzenberger and Benjamin Van Durme. 2021. Fac-\ntoring statutory reasoning as language understanding chal-\nlenges. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language Process-\ning (Volume 1: Long Papers) , pages 2742\u20132758, Online. As-\nsociation for Computational Linguistics.\nAlfred Horn. 1951. On Sentences Which are True of Di-\nrect Unions of Algebras. The Journal of Symbolic Logic ,\n16(1):14\u201321. Publisher: Association for Symbolic Logic.\nAbe Hou, William Jurayj, Nils Holzenberger, Andrew Blair-\nStanek, and Benjamin Van Durme. 2024. Gaps or Hallu-\ncinations? Scrutinizing Machine-Generated Legal Analysis\nfor Fine-grained Text Evaluations. In Proceedings of the\nNatural Legal Language Processing Workshop 2024 , pages\n280\u2013302, Miami, FL, USA. Association for Computational\nLinguistics.\nJiani Huang, Ziyang Li, Ilias Fountalis, and Mayur Naik.\n2021. Numerical reasoning over legal contracts via rela-\ntional database. In Workshop on Databases and AI .\nInternal Revenue Service. 2025. U.S. Department of the\nTreasury, IRS Announce Direct File as Permanent Free Tax\nFiling Option, All 50 States and D.C. Invited to Join in Fil-\ning Season 2025.\nWilliam Jurayj, Jeffrey Cheng, and Benjamin Van Durme.\n2025. Is that your final answer? test-time scaling improves\nselective question answering. In Proceedings of the 63rd\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers) , pages 636\u2013644, Vienna,\nAustria. Association for Computational Linguistics.\nWilliam Jurayj, William Rudman, and Carsten Eickhoff.\n2022. Garden path traversal in GPT-2. In Proceedings of the\nFifth BlackboxNLP Workshop on Analyzing and Interpret-\ning Neural Networks for NLP , pages 305\u2013313, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Computa-\ntional Linguistics.\nManuj Kant, Sareh Nabi, Manav Kant, Roland Scharrer,\nMegan Ma, and Marzieh Nabi. 2025. Towards Robust Le-\ngal Reasoning: Harnessing Logical LLMs in Law. arXiv\npreprint . ArXiv:2502.17638 [cs] version: 1.\nRandy Kozel. 2014. The Scope of Precedent. Michigan Law\nReview , 113(2):179\u2013230.\nDacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi\nMo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi,\nShishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, and\nIon Stoica. 2025. LLMs Can Easily Learn to Reason\nfrom Demonstrations Structure, not content, is what matters!\narXiv preprint . ArXiv:2502.07374 [cs].\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,\nMichele Bevilacqua, Fabio Petroni, and Percy Liang. 2024.\nLost in the Middle: How Language Models Use Long Con-\ntexts. Transactions of the Association for Computational\nLinguistics , 12:157\u2013173. Place: Cambridge, MA Publisher:\nMIT Press.Denis Merigoux. 2023. Experience report: implementing a\nreal-world, medium-sized program derived from a legisla-\ntive specification. Programming Languages and the Law .\nDenis Merigoux, Nicolas Chataing, and Jonathan Protzenko.\n2021a. Catala: a programming language for the law. Proc.\nACM Program. Lang. , 5(ICFP):77:1\u201377:29.\nDenis Merigoux, Rapha \u00a8el Monat, and Jonathan Protzenko.\n2021b. A modern compiler for the French tax code. In\nProceedings of the 30th ACM SIGPLAN International Con-\nference on Compiler Construction , CC 2021, pages 71\u201382,\nNew York, NY , USA. Association for Computing Machin-\nery.\nJason Morris. 2020. Blawx: Rules as Code\nDemonstration. MIT Computational Law Report .\nHttps://law.mit.edu/pub/blawxrulesascodedemonstration.\nOpenAI. 2025a. Introducing GPT-4.1 in the API.\nOpenAI. 2025b. Introducing OpenAI o3 and o4-mini.\nDebjit Paul, Robert West, Antoine Bosselut, and Boi Falt-\nings. 2024. Making reasoning matter: Measuring and\nimproving faithfulness of chain-of-thought reasoning. In\nFindings of the Association for Computational Linguistics:\nEMNLP 2024 , pages 15012\u201315032, Miami, Florida, USA.\nAssociation for Computational Linguistics.\nQwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan\nHui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,\nJianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Jun-\nyang Lin, and 24 others. 2025. Qwen2.5 Technical Report.\narXiv preprint . ArXiv:2412.15115 [cs].\nM. J. Sergot, F. Sadri, R. A. Kowalski, F. Kriwaczek,\nP. Hammond, and H. T. Cory. 1986. The British Nation-\nality Act as a logic program. Communications of the ACM ,\n29(5):370\u2013386. Publisher: Association for Computing Ma-\nchinery (ACM).\nD. M. Sherman. 1987. A Prolog model of the income tax\nact of Canada. In Proceedings of the 1st international con-\nference on Artificial intelligence and law , ICAIL \u201987, pages\n127\u2013136, New York, NY , USA. Association for Computing\nMachinery.\nD. M. Sherman. 1989. Expert systems and ICAI in tax law:\nkilling two birds with one AI stone. In Proceedings of the\n2nd international conference on Artificial intelligence and\nlaw, ICAIL \u201989, pages 74\u201380, New York, NY , USA. Asso-\nciation for Computing Machinery.\nRichard Shin and Benjamin Van Durme. 2022. Few-shot\nsemantic parsing with language models trained on code. In\nProceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 5417\u20135425, Seattle,\nUnited States. Association for Computational Linguistics.\nJoey Skaf, Luis Ibanez-Lissen, Robert McCarthy, Con-\nnor Watts, Vasil Georgiv, Hannes Whittingham, Lorena\nGonzalez-Manzano, David Lindner, Cameron Tice, Ed-\nward James Young, and Puria Radmard. 2025. Large\nlanguage models can learn and generalize steganographic\nchain-of-thought under process supervision. arXiv preprint .\nArXiv:2506.01926 [cs].\n\n--- Page 10 ---\nBenjamin Adin Spiegel, Ziyi Yang, William Jurayj, Ben\nBachmann, Stefanie Tellex, and George Konidaris. 2024.\nInforming reinforcement learning agents by grounding lan-\nguage to markov decision processes. In Workshop on Train-\ning Agents with Foundation Models at RLC 2024 .\nElias Stengel-Eskin, Kyle Rawlins, and Benjamin Van\nDurme. 2024. Zero and few-shot semantic parsing with am-\nbiguous inputs. In The Twelfth International Conference on\nLearning Representations .\nElias Stengel-Eskin and Benjamin Van Durme. 2023. Did\nYou Mean...? Confidence-based Trade-offs in Semantic\nParsing. In Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing , pages 2621\u2013\n2629, Singapore. Association for Computational Linguis-\ntics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2023. Self-consistency improves chain of\nthought reasoning in language models. In The Eleventh In-\nternational Conference on Learning Representations .\nOrion Weller, Benjamin Van Durme, Dawn Lawrie, Ash-\nwin Paranjape, Yuhao Zhang, and Jack Hessel. 2025a.\nPromptriever: Instruction-trained retrievers can be prompted\nlike language models. In The Thirteenth International Con-\nference on Learning Representations .\nOrion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates,\nDawn Lawrie, and Benjamin Van Durme. 2025b. Rank1:\nTest-Time Compute for Reranking in Information Retrieval.\narXiv preprint . ArXiv:2502.18418 [cs].\nJan Wielemaker, Tom Schrijvers, Markus Triska, and\nTorbj \u00a8orn Lager. 2010. SWI-Prolog. arXiv preprint .\nArXiv:1011.5332 [cs].\nEugene Yang, Andrew Yates, Kathryn Ricci, Orion Weller,\nVivek Chari, Benjamin Van Durme, and Dawn Lawrie.\n2025. Rank-K: Test-Time Reasoning for Listwise Rerank-\ning. arXiv preprint . ArXiv:2505.14432 [cs] version: 1.\nJay Yu, Kevin McCluskey, and Saikat Mukherjee. 2020. Tax\nKnowledge Graph for a Smarter and More Personalized Tur-\nboTax. arXiv preprint . ArXiv:2009.06103 [cs].\nYiqun Zhang, Hao Li, Jianhao Chen, Hangfan Zhang, Peng\nYe, Lei Bai, and Shuyue Hu. 2025. Beyond GPT-5: Making\nLLMs Cheaper and Better via Performance-Efficiency Opti-\nmized Routing. arXiv preprint . ArXiv:2508.12631 [cs].",
  "project_dir": "artifacts/projects/NeuroSymbolic_Tax_Reasoning_System",
  "communication_dir": "artifacts/projects/NeuroSymbolic_Tax_Reasoning_System/.agent_comm",
  "assigned_at": "2025-08-29T20:51:07.020186",
  "status": "assigned"
}